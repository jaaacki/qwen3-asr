# ==========================================
# ==========================================
# Qwen3-ASR Configuration (Environment Variables)
# ==========================================

# -----------------
# Compose & Network
# -----------------
# External port to expose on the host machine
PORT=8100

# -----------------
# Core Behavior
# -----------------
MODEL_ID=Qwen/Qwen3-ASR-1.7B
IDLE_TIMEOUT=1800
REQUEST_TIMEOUT=300

# -----------------
# Subtitle Aligner
# -----------------
FORCED_ALIGNER_ID=Qwen/Qwen3-ForcedAligner-0.6B

# -----------------
# Streaming / Websockets
# -----------------
# WS_BUFFER_SIZE=14400 
# WS_OVERLAP_SIZE=4800
# WS_FLUSH_SILENCE_MS=600

# -----------------
# Hardware Acceleration & Quantization
# -----------------
QUANTIZE=
# e.g., 'int8', 'fp8'
USE_CUDA_GRAPHS=false

# -----------------
# Speculative Decoding & Models
# -----------------
DUAL_MODEL=false
FAST_MODEL_ID=Qwen/Qwen3-ASR-0.6B
USE_SPECULATIVE=false

# -----------------
# Experimental Backends
# -----------------
USE_VLLM=false
USE_CAUSAL_ENCODER=false
ONNX_ENCODER_PATH=
TRT_ENCODER_PATH=

# -----------------
# Infrastructure
# -----------------
GATEWAY_MODE=true
# NUMA_NODE=0
USE_GRANIAN=false
WORKER_HOST=127.0.0.1
WORKER_PORT=8001

# -----------------
# Docker Environment Parameters
# -----------------
# These usually bypass caching but are good to configure system-wide allocations
PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True

# -----------------
# External Services (e.g. Translation Feature)
# If you pull the translation feature branch later, these are the API configs
OPENAI_API_KEY=your_api_key_here
OPENAI_BASE_URL=
TRANSLATE_MODEL=gpt-4o-mini
